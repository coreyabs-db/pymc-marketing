{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(mmm_time_slice_cross_validation)=\n",
    "# Time-Slice-Cross-Validation and Parameter Stability\n",
    "\n",
    "In this notebook we will illustrate how to perform time-slice cross validation for a media mix model. This is an important step to evaluate the stability and quality of the model. We not only look into out of sample predictions but also the stability of the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pymc_marketing.metrics import crps\n",
    "from pymc_marketing.mmm import (\n",
    "    MMM,\n",
    "    GeometricAdstock,\n",
    "    LogisticSaturation,\n",
    ")\n",
    "from pymc_marketing.mmm.utils import apply_sklearn_transformer_across_dim\n",
    "from pymc_marketing.paths import data_dir\n",
    "from pymc_marketing.prior import Prior\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 7]\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed: int = sum(map(ord, \"mmm\"))\n",
    "rng: np.random.Generator = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "We use the same data as in the example notebook {ref}`mmm_example`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = data_dir / \"mmm_example.csv\"\n",
    "\n",
    "data_df = pd.read_csv(data_path, parse_dates=[\"date_week\"])\n",
    "\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Time-Slice-Cross-Validation Strategy\n",
    "\n",
    "The main idea of the time-slice cross validation process is to fit the model on a time slice of the data and then evaluate it on the next time slice. We repeat this process for each time slice of the data. As we want to simulate a production-like environment where we enlarge our training data over time, we make the time-slice size grow over time.\n",
    "\n",
    "Following the strategy of the example notebook, we use the costs share of each channel to set the prior standard deviation of the beta parameters. We need to compute this share for each training time slice independently.\n",
    "\n",
    "```{admonition} Data Leakage\n",
    ":class: warning\n",
    "\n",
    "It is very important to avoid data leakage when performing time-slice cross validation. This means that the model should not see any training data from the future. This also includes any data pre-processing steps!\n",
    "\n",
    "For example, as mentioned above, we need to compute the costs share for each training time slice independently if we want to avoid data leakage. Other sources of data leakage include using a global feature for thr trend component. In our case, we simply use an increasing variable `t` so we are safe as we just increase it by one for each time slice.\n",
    "```\n",
    "\n",
    "We wrap the main steps of the training procedure in a set of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sigma_from_costs(\n",
    "    X: pd.DataFrame, channel_columns: list[str]\n",
    ") -> list[float]:\n",
    "    \"\"\"Compute the prior standard deviation of the beta parameters from the costs share of each channel.\"\"\"\n",
    "    n_channels = len(channel_columns)\n",
    "    total_spend_per_channel = X[channel_columns].sum(axis=0)\n",
    "    spend_share = total_spend_per_channel / total_spend_per_channel.sum()\n",
    "    prior_sigma = n_channels * spend_share.to_numpy()\n",
    "    return prior_sigma.tolist()\n",
    "\n",
    "\n",
    "def get_mmm(X: pd.DataFrame, channel_columns: list[str]) -> MMM:\n",
    "    \"\"\"Specify the model.\"\"\"\n",
    "    prior_sigma = compute_sigma_from_costs(X, channel_columns)\n",
    "\n",
    "    model_config = {\n",
    "        \"intercept\": Prior(\"Normal\", mu=0, sigma=0.5),\n",
    "        \"saturation_beta\": Prior(\"HalfNormal\", sigma=prior_sigma, dims=\"channel\"),\n",
    "        \"gamma_control\": Prior(\"Normal\", mu=0, sigma=0.05, dims=\"control\"),\n",
    "        \"likelihood\": Prior(\"Normal\", sigma=Prior(\"Exponential\", lam=1 / 10)),\n",
    "    }\n",
    "\n",
    "    return MMM(\n",
    "        adstock=GeometricAdstock(l_max=8),\n",
    "        saturation=LogisticSaturation(),\n",
    "        date_column=\"date_week\",\n",
    "        channel_columns=channel_columns,\n",
    "        control_columns=[\n",
    "            \"event_1\",\n",
    "            \"event_2\",\n",
    "            \"t\",\n",
    "        ],\n",
    "        yearly_seasonality=2,\n",
    "        model_config=model_config,\n",
    "    )\n",
    "\n",
    "\n",
    "def fit_mmm(\n",
    "    mmm: MMM, X: pd.DataFrame, y: pd.Series, random_seed: np.random.Generator\n",
    ") -> MMM:\n",
    "    \"\"\"Fit the model.\"\"\"\n",
    "    fit_kwargs = {\n",
    "        \"tune\": 1_000,\n",
    "        \"chains\": 4,\n",
    "        \"draws\": 1_000,\n",
    "        \"nuts_sampler\": \"numpyro\",\n",
    "        \"random_seed\": random_seed,\n",
    "    }\n",
    "    _ = mmm.fit(X, y, progressbar=False, **fit_kwargs)\n",
    "    _ = mmm.sample_posterior_predictive(\n",
    "        X, extend_idata=True, combined=True, progressbar=False, random_seed=random_seed\n",
    "    )\n",
    "    return mmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of convenience, we define a data container to store the results of the time-slice cross validation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TimeSliceCrossValidationResult:\n",
    "    \"\"\"Container for the results of the time slice cross validation.\"\"\"\n",
    "\n",
    "    X_train: pd.DataFrame\n",
    "    y_train: pd.Series\n",
    "    X_test: pd.DataFrame\n",
    "    y_test: pd.Series\n",
    "    mmm: MMM\n",
    "    y_pred_test: pd.Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the main function that performs the time-slice cross validation step by calling the functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_slice_cross_validation_step(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    random_seed: np.random.Generator,\n",
    ") -> TimeSliceCrossValidationResult:\n",
    "    \"\"\"Time-slice cross validation step.\n",
    "\n",
    "    We fit the model on the training data and generate predictions for the test data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: pd.DataFrame\n",
    "        Training data.\n",
    "    y_train: pd.Series\n",
    "        Training target.\n",
    "    X_test: pd.DataFrame\n",
    "        Test data.\n",
    "    y_test: pd.Series\n",
    "        Test target.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TimeSliceCrossValidationResult\n",
    "        Results of the time slice cross validation step.\n",
    "    \"\"\"\n",
    "    mmm = get_mmm(X_train, channel_columns=[\"x1\", \"x2\"])\n",
    "    mmm = fit_mmm(mmm, X_train, y_train, random_seed)\n",
    "\n",
    "    y_pred_test = mmm.sample_posterior_predictive(\n",
    "        X_pred=X_test,\n",
    "        include_last_observations=True,\n",
    "        original_scale=True,\n",
    "        extend_idata=False,\n",
    "        progressbar=False,\n",
    "        random_seed=random_seed,\n",
    "    )\n",
    "\n",
    "    return TimeSliceCrossValidationResult(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        mmm=mmm,\n",
    "        y_pred_test=y_pred_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run the time-slice cross validation loop 💪!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Time-Slice-Cross-Validation Loop\n",
    "\n",
    "Depending on the business requirements, we need to decide the initial number of observations to use for fitting the model (`n_init`) and the forecast horizon (`forecast_horizon`). For this example, we use the first 158 observations to fit the model and then predict the next 12 observations (3 months)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_df[[\"date_week\", \"x1\", \"x2\", \"event_1\", \"event_2\", \"t\"]]\n",
    "y = data_df[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_init = 158\n",
    "forecast_horizon = 12\n",
    "n_iterations = y.size - n_init - forecast_horizon + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results: list[TimeSliceCrossValidationResult] = []\n",
    "\n",
    "\n",
    "for iteration in tqdm(range(n_iterations)):\n",
    "    # Split data into train and test\n",
    "    train_test_split = iteration + n_init\n",
    "    X_train = X.iloc[:train_test_split]\n",
    "    y_train = y.iloc[:train_test_split]\n",
    "    X_test = X.iloc[train_test_split : train_test_split + forecast_horizon]\n",
    "    y_test = y.iloc[train_test_split : train_test_split + forecast_horizon]\n",
    "\n",
    "    step = time_slice_cross_validation_step(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        random_seed=rng,\n",
    "    )\n",
    "    results.append(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Diagnostics\n",
    "\n",
    "First, we evaluate whether we have any divergences in the model (we can extend the analysis more more model diagnostics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([result.mmm.idata[\"sample_stats\"][\"diverging\"].sum().item() for result in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have no divergences in the model 😃!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Parameter Stability\n",
    "\n",
    "Next, we look at the stability of the model parameters. For a good model, these should not change abruptly over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adstock Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "az.plot_forest(\n",
    "    data=[result.mmm.idata[\"posterior\"] for result in results],\n",
    "    model_names=[f\"Iteration {i}\" for i in range(n_iterations)],\n",
    "    var_names=[\"adstock_alpha\"],\n",
    "    combined=True,\n",
    "    ax=ax,\n",
    ")\n",
    "fig.suptitle(\"Adstock Alpha\", fontsize=18, fontweight=\"bold\", y=1.06);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saturation Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "az.plot_forest(\n",
    "    data=[result.mmm.idata[\"posterior\"] for result in results],\n",
    "    model_names=[f\"Iteration {i}\" for i in range(n_iterations)],\n",
    "    var_names=[\"saturation_beta\"],\n",
    "    combined=True,\n",
    "    ax=ax,\n",
    ")\n",
    "fig.suptitle(\"Saturation Beta\", fontsize=18, fontweight=\"bold\", y=1.06);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saturation Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "az.plot_forest(\n",
    "    data=[result.mmm.idata[\"posterior\"] for result in results],\n",
    "    model_names=[f\"Iteration {i}\" for i in range(n_iterations)],\n",
    "    var_names=[\"saturation_lam\"],\n",
    "    combined=True,\n",
    "    ax=ax,\n",
    ")\n",
    "fig.suptitle(\"Saturation Lambda\", fontsize=18, fontweight=\"bold\", y=1.06);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters seem to be stable over time. This implies that the estimates ROAS will not change abruptly over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Out of Sample Predictions\n",
    "\n",
    "Finally, we evaluate the out of sample predictions. To begin with, we can simply plot the posterior predictive distributions for each iteration for both the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    nrows=n_iterations,\n",
    "    ncols=1,\n",
    "    figsize=(12, 25),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    ax = axes[i]\n",
    "    result.mmm.plot_posterior_predictive(original_scale=True, ax=ax)\n",
    "\n",
    "    hdi_prob = 0.94\n",
    "    test_hdi = az.hdi(result.y_pred_test[\"y\"].to_numpy().T, hdi_prob=hdi_prob)\n",
    "\n",
    "    ax.fill_between(\n",
    "        result.X_test[\"date_week\"],\n",
    "        test_hdi[:, 0],\n",
    "        test_hdi[:, 1],\n",
    "        color=\"C1\",\n",
    "        label=f\"{hdi_prob:.0%} HDI (test)\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax.plot(X[\"date_week\"], y, marker=\"o\", color=\"black\")\n",
    "    ax.axvline(result.X_test[\"date_week\"].iloc[0], color=\"C2\", linestyle=\"--\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "axes[-1].set(xlim=(X[\"date_week\"].iloc[n_init - 9], None))\n",
    "fig.suptitle(\"Posterior Predictive Check\", fontsize=18, fontweight=\"bold\", y=1.02);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the out of sample predictions look very good 🚀!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quantify the model performance using the Continuous Ranked Probability Score (CRPS).\n",
    "\n",
    "> *“The CRPS — Continuous Ranked Probability Score — is a score function that compares a single ground truth value to a Cumulative Distribution Function. It can be used as a metric to evaluate a model’s performance when the target variable is continuous and the model predicts the target’s distribution; Examples include Bayesian Regression or Bayesian Time Series models.”*\n",
    "\n",
    "\n",
    "For a nice explanation of the CRPS, check out this [blog post](https://towardsdatascience.com/crps-a-scoring-function-for-bayesian-machine-learning-models-dd55a7a337a8).\n",
    "\n",
    "In PyMC Marketing, we provide the function {func}`crps <pymc_marketing.metrics.crps>` to compute this metric. We can use it to compute the CRPS score for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crps_results_train: list[float] = [\n",
    "    crps(\n",
    "        y_true=result.y_train.to_numpy(),\n",
    "        y_pred=az.extract(\n",
    "            # Scale the predictions back to the original scale\n",
    "            apply_sklearn_transformer_across_dim(\n",
    "                data=result.mmm.idata.posterior_predictive[\"y\"],\n",
    "                func=result.mmm.get_target_transformer().inverse_transform,\n",
    "                dim_name=\"date\",\n",
    "            )\n",
    "        )[\"y\"]\n",
    "        .to_numpy()\n",
    "        .T,\n",
    "    )\n",
    "    for result in results\n",
    "]\n",
    "\n",
    "\n",
    "crps_results_test: list[float] = [\n",
    "    crps(\n",
    "        y_true=result.y_test.to_numpy(),\n",
    "        y_pred=result.y_pred_test[\"y\"].to_numpy().T,\n",
    "    )\n",
    "    for result in results\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=2, ncols=1, figsize=(12, 7), sharex=True, sharey=False, layout=\"constrained\"\n",
    ")\n",
    "\n",
    "ax[0].plot(crps_results_train, marker=\"o\", color=\"C0\", label=\"train\")\n",
    "ax[0].set(ylabel=\"CRPS\", title=\"Train CRPS\")\n",
    "ax[1].plot(crps_results_test, marker=\"o\", color=\"C1\", label=\"test\")\n",
    "ax[1].set(xlabel=\"Iteration\", ylabel=\"CRPS\", title=\"Test CRPS\")\n",
    "fig.suptitle(\"CRPS for each iteration\", fontsize=18, fontweight=\"bold\", y=1.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event though the visual results look great, we see that the CRPS mildly decreases for the training data while it increases for the test data as we increase the size of the training data. This is a sign that we are overfitting the model to the training data. Some strategies to overcome this issue include using regularization techniques and re-evaluate the model specification. This should be an iterative process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w -p pymc_marketing,pytensor,numpyro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
