{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(mmm_roas)=\n",
    "# Case Study: Unobserved Confounders, ROAS and Lift Tests\n",
    "\n",
    "### The Importance of Calibration in Media Mix Models\n",
    "\n",
    "When working with Media Mix Models (MMMs), calibration with lift tests is not just a technical step — it's essential for making these models reliable and actionable. Lift tests serve as a reality check, grounding the model's predictions in real-world outcomes. This process is particularly important because MMMs often rely on data that is both noisy and limited in scope, making accurate predictions difficult.\n",
    "\n",
    "### Why MMMs Need to Be Causal Models\n",
    "\n",
    "The true power of MMMs lies in their ability to guide decisions based on potential changes in marketing strategy. For instance, a marketer might ask, \"What impact would reducing the budget for channel X by 50% have on overall sales?\" To answer such questions, MMMs must be developed as *causal models*—models that can predict the effects of specific interventions. \n",
    "\n",
    "Creating a causal model involves constructing a well-defined *directed acyclic graph* (DAG). A DAG is a visual representation of the relationships between different variables in the model, helping to ensure that the right covariates (or control variables) are included. This isn't just about throwing in every available piece of data; it's about selecting variables that genuinely contribute to understanding the causal relationships in your marketing activities.\n",
    "\n",
    "### The Challenge of Unobserved Confounders\n",
    "\n",
    "Even with a well-constructed DAG, unobserved confounders can pose a significant problem. These are variables that influence both the treatment (e.g., spending on a particular marketing channel) and the outcome (e.g., sales) but are not included in the model. Their omission can lead to biased estimates, which in turn can result in poor decision-making. Unfortunately, unobserved confounders are almost inevitable in any real-world scenario.\n",
    "\n",
    "### A Recent Innovation: Calibrating MMMs with Bayesian Priors and Lift Tests\n",
    "\n",
    "In theory, methods like instrumental variables can help address the issue of unobserved confounders. An instrumental variable is a variable that is correlated with the treatment but not with the outcome, except through the treatment. However, finding good instruments is notoriously difficult, making this solution less practical.\n",
    "\n",
    "An alternative, more practical solution is to calibrate the MMM using lift tests. Lift tests are experiments that measure the incremental effect of a marketing intervention, such as a temporary increase in ad spend on a specific channel. By calibrating the model with the results of these tests, we can reduce the bias introduced by unobserved confounders. Bayesian methods are particularly well-suited for this approach because they allow us to incorporate prior knowledge—information we have before analyzing the data—into the model. This can significantly improve the model's accuracy, even before we start crunching the numbers.\n",
    "\n",
    "A recent paper, [Media Mix Model Calibration With Bayesian Priors](https://research.google/pubs/media-mix-model-calibration-with-bayesian-priors/) by Zhang et al. (2024), proposes a novel approach to calibrating MMMs. Instead of using the traditional beta (or regression) coefficients for each channel, the authors suggest parametrizing the model in terms of *return on ad spend* (ROAS). \n",
    "\n",
    "The concept is straightforward: express the beta coefficients in terms of ROAS, and then substitute these into a standard MMM equation. This method simplifies the calibration process and can partially mitigate biases comming from unobserved confounders and noise in the data.\n",
    "\n",
    "### Practical Application: Implementing the Method in PyMC\n",
    "\n",
    "In the blog post [Media Mix Model and Experimental Calibration: A Simulation Study](https://juanitorduz.github.io/mmm_roas/), [Juan Orduz](https://juanitorduz.github.io) provides a comprehensive implementation of this method using PyMC. This work demonstrates how this calibration approach can yield more accurate estimates, even when dealing with unobserved confounders. However, despite its strengths, this method has a limitation: it doesn't automatically improve when additional lift tests are added. The authors of the paper suggest aggregating the results of multiple lift tests by taking a weighted average of the ROAS estimates from each experiment.\n",
    "\n",
    "### A New Approach: Refining MMMs with Custom Likelihood Functions\n",
    "\n",
    "While the method proposed by Zhang et al. is effective, it could be more efficient in leveraging the results of lift tests. Running these experiments requires significant planning and resources, so it's important to maximize their value.\n",
    "\n",
    "To address this, we propose an alternative approach. Instead of directly reparametrizing the model in terms of ROAS, we suggest adding additional likelihood functions based on the saturation curves of the marketing channels. Saturation curves describe how the effectiveness of a marketing channel diminishes as spending increases. By calibrating the model through these curves using lift test results, we can continuously refine our estimates as more experiments are conducted. This approach allows us to make better use of the data from lift tests, leading to more accurate and reliable MMMs over time.\n",
    "\n",
    "### Demonstrating the Approach: Results and Implementation\n",
    "\n",
    "In our notebook, we replicate the results from the blog post [Media Mix Model and Experimental Calibration: A Simulation Study](https://juanitorduz.github.io/mmm_roas/) using the PyMC-Marketing framework. This framework allows us to incorporate lift tests through custom likelihood functions, providing a more nuanced calibration of the MMM. By continuously refining our estimates as new data becomes available, this approach offers a dynamic and robust solution to the challenges posed by unobserved confounders in marketing mix modeling.\n",
    "\n",
    "For those interested in diving deeper, we encourage you to explore a detailed description of the method in our notebook {ref}`mmm_lift_test`, where we walk through the process step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import graphviz as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from pymc_marketing.hsgp_kwargs import HSGPKwargs\n",
    "from pymc_marketing.mmm import (\n",
    "    MMM,\n",
    "    GeometricAdstock,\n",
    "    LogisticSaturation,\n",
    ")\n",
    "from pymc_marketing.paths import data_dir\n",
    "from pymc_marketing.prior import Prior\n",
    "\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 7]\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed: int = sum(map(ord, \"mmm-roas\"))\n",
    "rng: np.random.Generator = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "We read the data, which is available in the `data` of our repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = data_dir / \"mmm_roas_data.csv\"\n",
    "\n",
    "raw_df = pd.read_csv(data_path, parse_dates=[\"date\"])\n",
    "\n",
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many columns in the dataset. We will explain the below. For now, that is important is for modeling purposes we will only use the `date`, `x1`, `x2` (channels)and `y` (target) columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = raw_df.copy().filter([\"date\", \"x1\", \"x2\", \"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generating Process  \n",
    "\n",
    "In the original [blog post](https://juanitorduz.github.io/mmm_roas/), the authors generate the data using the following DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "g = gr.Digraph()\n",
    "g.node(name=\"seasonality\", label=\"seasonality\", color=\"lightgray\", style=\"filled\")\n",
    "g.node(name=\"trend\", label=\"trend\")\n",
    "g.node(name=\"z\", label=\"z\", color=\"lightgray\", style=\"filled\")\n",
    "g.node(name=\"x1\", label=\"x1\", color=\"#2a2eec80\", style=\"filled\")\n",
    "g.node(name=\"x2\", label=\"x2\", color=\"#fa7c1780\", style=\"filled\")\n",
    "g.node(name=\"y\", label=\"y\", color=\"#328c0680\", style=\"filled\")\n",
    "g.edge(tail_name=\"seasonality\", head_name=\"x1\")\n",
    "g.edge(tail_name=\"z\", head_name=\"x1\")\n",
    "g.edge(tail_name=\"x1\", head_name=\"y\")\n",
    "g.edge(tail_name=\"seasonality\", head_name=\"y\")\n",
    "g.edge(tail_name=\"trend\", head_name=\"y\")\n",
    "g.edge(tail_name=\"z\", head_name=\"y\")\n",
    "g.edge(tail_name=\"x2\", head_name=\"y\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the effect of the `x1` and `x2` channels on the `y` variable. We have additional covariates like yearly seasonality and a a  non-linear trend component. In addition, there is an unobserved confounder `z` that affects both the channels and the target variable. This variable introduces a bias in the estimates if we do not account for it. However, for this problem we are going to assume that we do not have access to the `z` variable (hence, unobserved).\n",
    "\n",
    "In the `raw_df` we have all all the columns needed in the data generating process to obtain the target variable `y`. For example, the `x1_adstock_saturated` column is the result of applying the adstock function and then the saturation to the `x1` channel.\n",
    "\n",
    "The target variable `y` is generated as  \n",
    "\n",
    "```\n",
    "y = amplitude * (trend + seasonality + z + x1_effect + x2_effect + epsilon)\n",
    "```\n",
    "\n",
    "where `epsilon` is a Gaussian noise and `amplitude` is a set to $100$.\n",
    "\n",
    "The variables `y01` and `y02` are the target variable `y` without the effect of the `x1` and `x2` channels, respectively. Hence\n",
    "\n",
    "```\n",
    "y01 = {amplitude} * (trend + seasonality + z + x2_effect + epsilon)\n",
    "y02 = {amplitude} * (trend + seasonality + z + x1_effect + epsilon)\n",
    "```\n",
    "\n",
    " For details on the data generating process, please refer to the [blog post](https://juanitorduz.github.io/mmm_roas/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the variables `y01` and `y02` we can compute the true ROAS for the `x1` and `x2` channels for the whole period ( for simplicity we ignore the carryover effect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_roas_x1 = (raw_df[\"y\"] - raw_df[\"y01\"]).sum() / raw_df[\"x1\"].sum()\n",
    "true_roas_x2 = (raw_df[\"y\"] - raw_df[\"y02\"]).sum() / raw_df[\"x2\"].sum()\n",
    "\n",
    "print(f\"True ROAS for x1: {true_roas_x1:.2f}\")\n",
    "print(f\"True ROAS for x2: {true_roas_x2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to recover the true ROAS for the `x1` and `x2` channels using a media mix model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before jumping into the model, let's plot channels the data to understand it better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = model_df.melt(\n",
    "    id_vars=[\"date\"], value_vars=[\"x1\", \"x2\"], var_name=\"channel\", value_name=\"x\"\n",
    ").pipe(\n",
    "    (sns.relplot, \"data\"),\n",
    "    kind=\"line\",\n",
    "    x=\"date\",\n",
    "    y=\"x\",\n",
    "    hue=\"channel\",\n",
    "    height=6,\n",
    "    aspect=2,\n",
    ")\n",
    "g.fig.suptitle(\"Channels Spend Data\", fontsize=16, y=1.02);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "To begin with, we fit a media mix model, without including the unobserved confounder `z`, using a saturated adstock and logistic saturation using PyMC-Marketing's API. For more details on the model, please refer to the {ref}`mmm_example` notebook. As we discuss in that notebook, without any lift test information, a good starting point is to pass the cost share into the prior of the beta channel coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_share = model_df[[\"x1\", \"x2\"]].sum() / model_df[[\"x1\", \"x2\"]].sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify the model configuration as follows (see the {ref}`model_configuration` notebook for more details): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"intercept\": Prior(\"Normal\", mu=200, sigma=20),\n",
    "    \"likelihood\": Prior(\"Normal\", sigma=Prior(\"HalfNormal\", sigma=2)),\n",
    "    \"gamma_fourier\": Prior(\"Normal\", mu=0, sigma=2, dims=\"fourier_mode\"),\n",
    "    \"intercept_tvp_config\": HSGPKwargs(\n",
    "        m=100, L=None, eta_lam=1.0, ls_mu=5.0, ls_sigma=10.0, cov_func=None\n",
    "    ),\n",
    "    \"adstock_alpha\": Prior(\"Beta\", alpha=2, beta=3, dims=\"channel\"),\n",
    "    \"saturation_lam\": Prior(\"Gamma\", alpha=2, beta=2, dims=\"channel\"),\n",
    "    \"saturation_beta\": Prior(\"HalfNormal\", sigma=cost_share.to_numpy(), dims=\"channel\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that we are going to use a Gaussian process to model the non-linear trend component (see {ref}`mmm_tv_intercept`).\n",
    "\n",
    "Let's fit the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mmm = MMM(\n",
    "    adstock=GeometricAdstock(l_max=4),\n",
    "    saturation=LogisticSaturation(),\n",
    "    date_column=\"date\",\n",
    "    channel_columns=[\"x1\", \"x2\"],\n",
    "    time_varying_intercept=True,\n",
    "    time_varying_media=False,\n",
    "    yearly_seasonality=5,\n",
    "    model_config=model_config,\n",
    ")\n",
    "\n",
    "y = model_df[\"y\"]\n",
    "X = model_df.drop(columns=[\"y\"])\n",
    "\n",
    "fit_kwargs = {\n",
    "    \"tune\": 1_500,\n",
    "    \"chains\": 4,\n",
    "    \"draws\": 2_000,\n",
    "    \"nuts_sampler\": \"numpyro\",\n",
    "    \"target_accept\": 0.92,\n",
    "    \"random_seed\": rng,\n",
    "}\n",
    "\n",
    "_ = mmm.fit(X, y, **fit_kwargs)\n",
    "_ = mmm.sample_posterior_predictive(\n",
    "    X, extend_idata=True, combined=True, random_seed=rng\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that we do not have divergent transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of diverging samples\n",
    "mmm.idata[\"sample_stats\"][\"diverging\"].sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look into the model components contributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmm.plot_components_contributions(original_scale=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look very similar to the results from the original blog post. In particular, note that we were able to capture the non-linear trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look into the channel contributions against the true effects (which we know from the data generating process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_contribution_original_scale = mmm.compute_channel_contribution_original_scale()\n",
    "channels_contribution_original_scale_hdi = az.hdi(\n",
    "    ary=channels_contribution_original_scale\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=2, figsize=(15, 8), ncols=1, sharex=True, sharey=False, layout=\"constrained\"\n",
    ")\n",
    "\n",
    "amplitude = 100\n",
    "\n",
    "for i, x in enumerate([\"x1\", \"x2\"]):\n",
    "    # HDI estimated contribution in the original scale\n",
    "    ax[i].fill_between(\n",
    "        x=model_df[\"date\"],\n",
    "        y1=channels_contribution_original_scale_hdi.sel(channel=x)[\"x\"][:, 0],\n",
    "        y2=channels_contribution_original_scale_hdi.sel(channel=x)[\"x\"][:, 1],\n",
    "        color=f\"C{i}\",\n",
    "        label=rf\"{x} $94\\%$ HDI contribution\",\n",
    "        alpha=0.4,\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        x=\"date\",\n",
    "        y=f\"{x}_effect\",\n",
    "        data=raw_df.assign(**{f\"{x}_effect\": lambda df: amplitude * df[f\"{x}_effect\"]}),  # noqa B023\n",
    "        color=f\"C{i}\",\n",
    "        label=f\"{x} effect\",\n",
    "        ax=ax[i],\n",
    "    )\n",
    "\n",
    "    ax[i].legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    ax[i].set(title=f\"Channel {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the contribution for `x1` is very different from the true effect. This is because the absence of the unobserved confounder `z`. For `x2`, the contribution is very similar to the true effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute the ROAS for the `x1` and `x2` channels (again, ignoring the small carryover effect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roas_posterior = (\n",
    "    channels_contribution_original_scale.sum(dim=\"date\")\n",
    "    / model_df[[\"x1\", \"x2\"]].sum().to_numpy()[np.newaxis, np.newaxis, :]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=2, ncols=1, figsize=(12, 7), sharex=True, sharey=False, layout=\"constrained\"\n",
    ")\n",
    "\n",
    "az.plot_posterior(roas_posterior, ref_val=[true_roas_x1, true_roas_x2], ax=ax)\n",
    "ax[0].set_title(\"x1\")\n",
    "ax[1].set_title(\"x2\")\n",
    "fig.suptitle(\"ROAS Posterior Distribution\", fontsize=16, y=1.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the ROAS for `x1` is very different from the true value. This is reflecting the bias induced by the unobserved confounder `z`. The models suggests that `x1` is more effective than `x2`, but we know from the data generating process that `x2` is more effective!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lift Test Model\n",
    "\n",
    "Now we fit a model with some lift tests. We will use the same model configuration as before, but we free the priors of the beta channel coefficients as these are included in the saturation function parametrization. In general, we expect lift tests priors or associated custom likelihoods to be better than the cost share prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"intercept\": Prior(\"Normal\", mu=200, sigma=20),\n",
    "    \"likelihood\": Prior(\"Normal\", sigma=Prior(\"HalfNormal\", sigma=2)),\n",
    "    \"gamma_fourier\": Prior(\"Normal\", mu=0, sigma=2, dims=\"fourier_mode\"),\n",
    "    \"intercept_tvp_config\": HSGPKwargs(\n",
    "        m=50, L=None, eta_lam=1.0, ls_mu=5.0, ls_sigma=10.0, cov_func=None\n",
    "    ),\n",
    "    \"adstock_alpha\": Prior(\"Beta\", alpha=2, beta=3, dims=\"channel\"),\n",
    "    \"saturation_lam\": Prior(\"Gamma\", alpha=2, beta=2, dims=\"channel\"),\n",
    "    \"saturation_beta\": Prior(\"HalfNormal\", sigma=1.5, dims=\"channel\"),\n",
    "}\n",
    "\n",
    "mmm_lift = MMM(\n",
    "    adstock=GeometricAdstock(l_max=4),\n",
    "    saturation=LogisticSaturation(),\n",
    "    date_column=\"date\",\n",
    "    channel_columns=[\"x1\", \"x2\"],\n",
    "    time_varying_intercept=True,\n",
    "    time_varying_media=False,\n",
    "    yearly_seasonality=5,\n",
    "    model_config=model_config,\n",
    ")\n",
    "\n",
    "# we need to build the model before adding the lift test measurements\n",
    "mmm_lift.build_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Lift Tests\n",
    ":class: tip\n",
    "\n",
    "In a lift study, one temporarily changes the budget of a channel for a fixed period of time, and then uses some method (for example [CausalPy](https://causalpy.readthedocs.io/en/stable/)) to make inference about the change in sales directly caused by the adjustment.\n",
    "\n",
    "A lift test is characterized by:\n",
    "\n",
    "- `channel`: the channel that was tested\n",
    "- `x`: pre-test channel spend\n",
    "- `delta_x`: change made to x\n",
    "- `delta_y`: inferred change in sales due to delta_x\n",
    "- `sigma`: standard deviation of delta_y\n",
    "\n",
    "An experiment characterized in this way can be viewed as two points on the saturation curve for the channel. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next assume we have ran two lift tests for the `x1` and `x2` channels. The results table looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lift_test = pd.DataFrame(\n",
    "    data={\n",
    "        \"channel\": [\"x1\", \"x2\", \"x1\", \"x2\"],\n",
    "        \"x\": [0.25, 0.1, 0.8, 0.25],\n",
    "        \"delta_x\": [0.25, 0.1, 0.8, 0.25],\n",
    "        \"delta_y\": [\n",
    "            true_roas_x1 * 0.25,\n",
    "            true_roas_x2 * 0.1,\n",
    "            true_roas_x1 * 0.8,\n",
    "            true_roas_x2 * 0.25,\n",
    "        ],\n",
    "        \"sigma\": [3, 3, 3, 3],\n",
    "    }\n",
    ")\n",
    "\n",
    "df_lift_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comparison with the original blog post\n",
    ":class: tip\n",
    "\n",
    "Note that we have added the true ROAS for the `x1` and `x2` channels implicit to the `df_lift_test` table. We add them multiplying the `delta_y` as this is what we would have observed if we had run the lift test (or similar values).\n",
    "\n",
    "In the simulation [Media Mix Model and Experimental Calibration: A Simulation Study](https://juanitorduz.github.io/mmm_roas/), the author included these \"true\" values into the prior for the ROAS.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we fit the model with the lift test measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmm_lift.add_lift_test_measurements(df_lift_test=df_lift_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = mmm_lift.fit(X, y, **fit_kwargs)\n",
    "_ = mmm_lift.sample_posterior_predictive(\n",
    "    X, extend_idata=True, combined=True, random_seed=rng\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we verify that we do not have divergent transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of diverging samples\n",
    "mmm_lift.idata[\"sample_stats\"][\"diverging\"].sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the components contributions as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmm_lift.plot_components_contributions(original_scale=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we have recovered the non-linear trend component and the yearly seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the channel contributions to the true ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_contribution_original_scale = (\n",
    "    mmm_lift.compute_channel_contribution_original_scale()\n",
    ")\n",
    "channels_contribution_original_scale_hdi = az.hdi(\n",
    "    ary=channels_contribution_original_scale\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=2, figsize=(15, 8), ncols=1, sharex=True, sharey=False, layout=\"constrained\"\n",
    ")\n",
    "\n",
    "amplitude = 100\n",
    "\n",
    "for i, x in enumerate([\"x1\", \"x2\"]):\n",
    "    # HDI estimated contribution in the original scale\n",
    "    ax[i].fill_between(\n",
    "        x=model_df[\"date\"],\n",
    "        y1=channels_contribution_original_scale_hdi.sel(channel=x)[\"x\"][:, 0],\n",
    "        y2=channels_contribution_original_scale_hdi.sel(channel=x)[\"x\"][:, 1],\n",
    "        color=f\"C{i}\",\n",
    "        label=rf\"{x} $94\\%$ HDI contribution\",\n",
    "        alpha=0.4,\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        x=\"date\",\n",
    "        y=f\"{x}_effect\",\n",
    "        data=raw_df.assign(**{f\"{x}_effect\": lambda df: amplitude * df[f\"{x}_effect\"]}),  # noqa B023\n",
    "        color=f\"C{i}\",\n",
    "        label=f\"{x} effect\",\n",
    "        ax=ax[i],\n",
    "    )\n",
    "\n",
    "    ax[i].legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    ax[i].set(title=f\"Channel {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contributions look much better and they are very close to the ones of the original blog post! Hence, **these two approaches are very similar**. However note that the PyMC-Marketing approach is more flexible as it allows to enrich the estimates with more test and different media spends to have a better understanding of the saturation effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compute the ROAS for the `x1` and `x2` channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roas_posterior = (\n",
    "    channels_contribution_original_scale.sum(dim=\"date\")\n",
    "    / model_df[[\"x1\", \"x2\"]].sum().to_numpy()[np.newaxis, np.newaxis, :]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=2, ncols=1, figsize=(12, 7), sharex=True, sharey=False, layout=\"constrained\"\n",
    ")\n",
    "\n",
    "az.plot_posterior(roas_posterior, ref_val=[true_roas_x1, true_roas_x2], ax=ax)\n",
    "ax[0].set_title(\"x1\")\n",
    "ax[1].set_title(\"x2\")\n",
    "fig.suptitle(\"ROAS Posterior Distribution\", fontsize=16, y=1.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimates are very very close to the true ROAS! We do get from the model that `x2` is more effective than `x1`, which is aligned with the lift test results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have seen a concrete example of how media mix models can provide bias estimates when we have unobserved confounders in the model specification. Ideally, we'd add the confounder, but in the absence of that, we need to provide a reality anchor to the model to have meaningful estimates. We have shown that the PyMC-Marketing approach of adding lift test measurements to the model is very similar to the one proposed in the paper [Media Mix Model Calibration With Bayesian Priors](https://research.google/pubs/media-mix-model-calibration-with-bayesian-priors/) and the blog post [Media Mix Model and Experimental Calibration: A Simulation Study](https://juanitorduz.github.io/mmm_roas/).\n",
    "However, the PyMC-Marketing approach is more flexible as it allows enriching the estimates with more lift tests and different media spending to better understand the saturation effect. We have also seen why it is essential to include the lift test measurements in the model to account for the unobserved confounders and better understand the saturation effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w -p pymc_marketing,pytensor,numpyro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
